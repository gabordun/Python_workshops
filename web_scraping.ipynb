{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOuRT/r5ABQzBoUkuD06uZ3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabordun/Python_workshops/blob/master/web_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivafqztuo-Os",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import packages\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "#import requests\n",
        "from urllib import request\n",
        "import requests\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "seed_urls = ['https://welt.de',\n",
        "             'https://nytimes.com',\n",
        "             'https://theguardian.com',\n",
        "             'https://lefigaro.fr',\n",
        "             'https://elpais.com']\n",
        "\n",
        "for url in seed_urls:\n",
        "  basic=requests.get(url).text\n",
        "  soup=BeautifulSoup(basic, 'html.parser')\n",
        "\n",
        "  #b = soup.prettify()\n",
        "  #print(b)\n",
        "\n",
        "  keywords={'corona','Corona','coronavirus','corona virus','Corona Virus', 'Coronavirus'}\n",
        "\n",
        "  for x in keywords:\n",
        "   x = soup(text=re.compile('x'))\n",
        "   rowcounter = 0\n",
        "   for item in x:\n",
        "    rowcounter += 1\n",
        "  \n",
        "  y = soup.find_all('a')\n",
        "  colcounter = 0\n",
        "  for item in y:\n",
        "    colcounter += 1\n",
        "\n",
        "  print(rowcounter)\n",
        "  print(colcounter)\n",
        "\n",
        "  result = []\n",
        "  for i in range(0,colcounter):\n",
        "    a=y[i].get_text(\"title\").replace(\"title\",\" \").replace(\"\\n\",\" \")\n",
        "    result.append(a)\n",
        " \n",
        "  df = pd.DataFrame(data = result)\n",
        "  print(df)\n",
        "\n",
        "#df.to_csv(\"formula1.csv\")\n",
        "#uploaded = drive.CreateFile({'title': 'formula1.csv'})\n",
        "#uploaded.SetContentFile(\"formula1.csv\")\n",
        "#uploaded.Upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqI4wkyB9OMN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "6d9f8304-cdda-4d46-a223-67a46cd577cc"
      },
      "source": [
        "# finding the web pages\n",
        "\n",
        "source_pages=requests.get('http://www.onlinenewspapers.com/Top50/top50-by-country.htm')\n",
        "source=BeautifulSoup(source_pages, 'html.parser')\n",
        "\n",
        "c=source.prettify()\n",
        "c\n",
        "\n",
        "#for link in source.find_all('a'):\n",
        " #   print(link.get('href'))\n",
        "\n",
        "\n",
        "\n",
        "#seed_urls = ['https://welt.de',\n",
        "       #      'https://nytimes.com',\n",
        "        #     'https://theguardian.com',\n",
        "         #    'https://lefigaro.fr',\n",
        "          #   'https://elpais.com']"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-396319367ec9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msource_pages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://www.onlinenewspapers.com/Top50/top50-by-country.htm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_pages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprettify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'read'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m        \u001b[0;31m# It's a file-type object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mmarkup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         elif len(markup) <= 256 and (\n\u001b[0m\u001b[1;32m    247\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34mb'<'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m'<'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'Response' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t60DWdLQpTfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run counter\n",
        "\n",
        "for url in seed_urls:\n",
        "  basic=requests.get(url).text\n",
        "  soup=BeautifulSoup(basic, 'html.parser')\n",
        "\n",
        "  #b = soup.prettify()\n",
        "  #print(b)\n",
        "\n",
        "  keywords={'corona','Corona','coronavirus','corona virus','Corona Virus', 'Coronavirus'}\n",
        "\n",
        "  for x in keywords:\n",
        "   x = soup(text=re.compile('x'))\n",
        "   rowcounter = 0\n",
        "   for item in x:\n",
        "    rowcounter += 1\n",
        "  \n",
        "  y = soup.find_all('a')\n",
        "  colcounter = 0\n",
        "  for item in y:\n",
        "    colcounter += 1\n",
        "\n",
        "  print(rowcounter)\n",
        "  print(colcounter)\n",
        "\n",
        "  result = []\n",
        "  for i in range(0,colcounter):\n",
        "    a=y[i].get_text(\"title\").replace(\"title\",\" \").replace(\"\\n\",\" \")\n",
        "    result.append(a)\n",
        " \n",
        "  df = pd.DataFrame(data = result)\n",
        "  print(df)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}